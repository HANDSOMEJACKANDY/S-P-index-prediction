{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is an algorithm development playground for UBS Quant contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T21:15:16.695455Z",
     "start_time": "2018-10-11T21:15:13.938145Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPooling1D, Flatten, TimeDistributed, Concatenate\n",
    "from keras.layers import GaussianNoise, BatchNormalization, Dropout\n",
    "from keras.layers import Activation, merge, Input, concatenate, Reshape, add\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import Callback, LambdaCallback, TensorBoard, ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.utils.vis_utils import plot_model \n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T21:15:16.897542Z",
     "start_time": "2018-10-11T21:15:16.786497Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_window_normalize(df, moving_window=30, future_window=14, do_std=False):\n",
    "    \"\"\"\n",
    "    use a moving window of size moving_window to calculate the mean and std of data,\n",
    "    use these values as the estimated mean and std for next furture_window no. of days\n",
    "    \n",
    "    return processed df and the start index of the df \n",
    "        (data before start index is not normalized and should be pruned later)\n",
    "    \"\"\"\n",
    "    for i in range(len(df) - future_window, moving_window-1, -future_window): \n",
    "        # the sequence is going from the end to avoid creating intemdiate data storage\n",
    "        mean = df.iloc[i-moving_window:i, :].mean()\n",
    "        df.iloc[i:i+future_window, :] -= mean\n",
    "        \n",
    "        # not sure if we should comepare the change with std\n",
    "        if do_std:\n",
    "            df.iloc[i:i+future_window, :] /= df.iloc[i-moving_window:i, :].std()\n",
    "        else:\n",
    "            df.iloc[i:i+future_window, :] /= mean\n",
    "            \n",
    "    # return from the frist index that is normalized\n",
    "    return df, i\n",
    "\n",
    "def get_quarter_change_rate(df):\n",
    "    \"\"\"\n",
    "    for factor that change every quarter, the way to deal with that is to use the first order derivative directly\n",
    "    \"\"\"\n",
    "    this_quarter = df.iloc[0, :]\n",
    "    start_index = -1\n",
    "    for i in range(1, len(df)):\n",
    "        cur_row = df.iloc[i, :]\n",
    "        if not (cur_row == this_quarter).all():\n",
    "            # update start index if necessary\n",
    "            if start_index == -1:\n",
    "                start_index = i\n",
    "            \n",
    "            # update prev_quarter and this_quarter\n",
    "            prev_quarter = this_quarter.copy()\n",
    "            this_quarter = cur_row.copy()\n",
    "            \n",
    "            # update df\n",
    "            df.iloc[i, :] -= prev_quarter\n",
    "            df.iloc[i, :] /= prev_quarter\n",
    "            \n",
    "            # store new current value for copy\n",
    "            new_cur_row = df.iloc[i, :].copy()\n",
    "            \n",
    "        elif \"new_cur_row\" in locals(): \n",
    "            # if no update, duplicate the most recent none zero row\n",
    "            df.iloc[i, :] = new_cur_row\n",
    "            \n",
    "    return df, start_index\n",
    "\n",
    "def label_process(df, decision_window=14):\n",
    "    \"\"\"\n",
    "    process labels in another way:\n",
    "    average every 2 weeks and see the growth rate\n",
    "    \"\"\"\n",
    "    for i in range(len(df) - decision_window, decision_window-1, -decision_window):\n",
    "        mean = df.iloc[i-decision_window:i, :].mean()\n",
    "        df.iloc[i:i+decision_window, :] -= mean\n",
    "        df.iloc[i:i+decision_window, :] /= mean\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T21:15:16.991989Z",
     "start_time": "2018-10-11T21:15:16.971376Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_update_rate(df, n=5):\n",
    "    update_rate = []\n",
    "    for i in range(len(df.columns)):\n",
    "        counter_start = 0\n",
    "        min_counter = len(df) + 1\n",
    "        for j in range(n): # go for n loops to verify the update rate\n",
    "            counter = 0\n",
    "            start = df.iloc[counter_start, i]\n",
    "            while True:\n",
    "                counter += 1\n",
    "                if not start == df.iloc[counter_start+counter, i]:\n",
    "                    break\n",
    "            counter_start = counter + counter_start\n",
    "            if counter < min_counter:\n",
    "                min_counter = counter\n",
    "            if counter_start >= len(df):\n",
    "                break\n",
    "        update_rate.append(min_counter)\n",
    "    return update_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T21:15:17.205525Z",
     "start_time": "2018-10-11T21:15:17.073153Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def windowed_return_rate(df, window_size):\n",
    "    \"\"\"\n",
    "    use the mean of this window_size, and previous window_size to calcualte a stablized return rate\n",
    "    \n",
    "    the start_index is window_size\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(window_size, len(df)-window_size):\n",
    "        prev_mean = df.iloc[i-window_size:i, :].mean()\n",
    "        cur_mean = df.iloc[i:i+window_size, :].mean()\n",
    "        out.iloc[counter, :] = (cur_mean - prev_mean) / prev_mean\n",
    "        \n",
    "        if not (out.iloc[counter, :] <= 2).all() or not (out.iloc[counter, :] > -0.8).all():\n",
    "#             print(out.iloc[counter, :])\n",
    "            out.iloc[counter, :] = good_row.copy()\n",
    "        else:\n",
    "            good_row = out.iloc[counter, :]\n",
    "            \n",
    "        counter += 1\n",
    "    return out.iloc[:counter, :], window_size\n",
    "\n",
    "def get_quarter_change_rate(df):\n",
    "    \"\"\"\n",
    "    for factor that change every quarter, the way to deal with that is to use the first order derivative directly\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    this_quarter = df.iloc[0, :]\n",
    "    start_index = -1\n",
    "    for i in range(1, len(df)):\n",
    "        cur_row = df.iloc[i, :]\n",
    "        if not (cur_row == this_quarter).all():\n",
    "            # update start index if necessary\n",
    "            if start_index == -1:\n",
    "                start_index = i\n",
    "            \n",
    "            # update prev_quarter and this_quarter\n",
    "            prev_quarter = this_quarter.copy()\n",
    "            this_quarter = cur_row.copy()\n",
    "            \n",
    "            # update df\n",
    "            df.iloc[i, :] -= prev_quarter\n",
    "            df.iloc[i, :] /= prev_quarter\n",
    "            \n",
    "            # store new current value for copy\n",
    "            new_cur_row = df.iloc[i, :].copy()\n",
    "\n",
    "        elif \"new_cur_row\" in locals(): \n",
    "            # if no update, duplicate the most recent none zero row\n",
    "            df.iloc[i, :] = new_cur_row\n",
    "            \n",
    "    for i in range(start_index, len(df)):\n",
    "        if df.iloc[i, :].max() <= 1:\n",
    "            start_index = i\n",
    "            # make sure all unprocessed rows are pruned\n",
    "            df = df.iloc[i:, :]\n",
    "            break\n",
    "            \n",
    "    return df, start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T21:15:17.326011Z",
     "start_time": "2018-10-11T21:15:17.295515Z"
    }
   },
   "outputs": [],
   "source": [
    "# data generator for training and validation of stateless lstm\n",
    "def generate_train_data(df1, df2, df3, label, test_split=0.2, valid_split=0.1, seq_len=365):\n",
    "    # print out data information\n",
    "    print(len(df1.columns), len(df2.columns), len(df3.columns), len(label.columns))\n",
    "    print(len(df1), len(df2), len(df3), len(label))\n",
    "    \n",
    "    # get shortest data length (assuming all data start at the same time)\n",
    "    data_len = min(len(df1), len(df2), len(df3), len(label))-seq_len\n",
    "    test_split_len = int(data_len*(1-test_split-valid_split))\n",
    "    valid_split_len = int(data_len*(1-valid_split))\n",
    "    \n",
    "    # create sequence\n",
    "    df1 = np.array([np.array(df1.iloc[i:i+seq_len, :]) for i in range(data_len)])\n",
    "    df2 = np.array([np.array(df2.iloc[i:i+seq_len, :]) for i in range(data_len)])\n",
    "    df3 = np.array([np.array(df3.iloc[i:i+seq_len, :]) for i in range(data_len)])\n",
    "    label = np.array([np.array(label.iloc[i+seq_len, :]) for i in range(data_len)]) # watch out for the shift here\n",
    "    \n",
    "    # create test split\n",
    "    return {\"train_df1\": df1[:test_split_len], \"train_df2\": df2[:test_split_len], \"train_df3\": df3[:test_split_len], \"train_label\": label[:test_split_len], \\\n",
    "           \"test_df1\": df1[test_split_len:valid_split_len], \"test_df2\": df2[test_split_len:valid_split_len], \"test_df3\": df3[test_split_len:valid_split_len], \"test_label\": label[test_split_len:valid_split_len],\n",
    "           \"valid_df1\": df1[valid_split_len:], \"valid_df2\": df2[valid_split_len:], \"valid_df3\": df3[valid_split_len:], \"valid_label\": label[valid_split_len:]\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T21:15:36.422977Z",
     "start_time": "2018-10-11T21:15:17.398682Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the factor time data\n",
    "df = pd.read_csv(\"Dataset_1003.csv\", index_col=[0])\n",
    "\n",
    "# seperate S&P label out\n",
    "label = df.iloc[:, :4].copy()\n",
    "df = df.iloc[:, 4:].copy() # preserve them as the input data\n",
    "\n",
    "# collect col index for daily updated factors and quarterly updated factors\n",
    "rate = get_update_rate(df)\n",
    "slow_update_index = [i for i, r in enumerate(rate) if r > 1 and i not in [32]]\n",
    "quick_update_index = [i for i, r in enumerate(rate) if r == 1]\n",
    "\n",
    "# seperate slowly updated factor and quickly updated factor\n",
    "slow_factor_df = df.iloc[:, slow_update_index]\n",
    "quick_factor_df = df.iloc[:, quick_update_index]\n",
    "\n",
    "# process label\n",
    "label, label_start_index = windowed_return_rate(label, window_size=14)\n",
    "\n",
    "# process fast updated factor\n",
    "quick_factor_df, quick_start_index = windowed_return_rate(quick_factor_df, window_size=14)\n",
    "\n",
    "# process slowly updated factor\n",
    "slow_factor_df, slow_start_index = get_quarter_change_rate(slow_factor_df)\n",
    "\n",
    "# determine which row the start with\n",
    "start_index = max(slow_start_index, quick_start_index, label_start_index)\n",
    "\n",
    "# remove the rows before start_index\n",
    "label = label.iloc[start_index-label_start_index:, :]\n",
    "quick_factor_df = quick_factor_df.iloc[start_index-quick_start_index:, :]\n",
    "slow_factor_df = slow_factor_df.iloc[start_index-slow_start_index:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T21:29:00.720589Z",
     "start_time": "2018-10-11T21:28:56.314768Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 19 4 4\n",
      "4465 4451 4451 4451\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 200, 25)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 200, 19)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 200, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 200, 128)     3328        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 200, 128)     2560        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 200, 2)       10          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 200, 128)     0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 200, 128)     0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 200, 2)       0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 200, 64)      49408       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 200, 64)      49408       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 200, 16)      1216        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 200, 144)     0           lstm_5[0][0]                     \n",
      "                                                                 lstm_6[0][0]                     \n",
      "                                                                 lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 64)           53504       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          33280       lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 512)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 4)            2052        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 194,766\n",
      "Trainable params: 194,766\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "seq_len = 200\n",
    "train_data = generate_train_data(slow_factor_df, quick_factor_df, label, label, seq_len=seq_len, test_split=0.2)\n",
    "try_normalization = False\n",
    "\n",
    "# define three inputs\n",
    "slow_feature_input = Input(shape=(seq_len, 25))\n",
    "quick_feature_input = Input(shape=(seq_len, 19))\n",
    "prev_label_input = Input(shape=(seq_len, 4))\n",
    "\n",
    "# flow for slow_feature_input\n",
    "x = Dense(units=128, activation=\"relu\")(slow_feature_input)\n",
    "if try_normalization:\n",
    "    x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(units=64, dropout=0.2, activation=\"relu\", return_sequences=True)(x)\n",
    "if try_normalization:\n",
    "    x = BatchNormalization()(x)\n",
    "slow_feature = x\n",
    "\n",
    "# flow for quick_feature_input\n",
    "x = Dense(units=128, activation=\"relu\")(quick_feature_input)\n",
    "if try_normalization:\n",
    "    x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(units=64, dropout=0.2, activation=\"relu\", return_sequences=True)(x)\n",
    "if try_normalization:\n",
    "    x = BatchNormalization()(x)\n",
    "quick_feature = x\n",
    "\n",
    "# flow for prev_label_input\n",
    "x = Dense(units=2, activation=\"relu\")(prev_label_input)\n",
    "if try_normalization:\n",
    "    x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(units=16, dropout=0.2, activation=\"relu\", return_sequences=True)(x)\n",
    "if try_normalization:\n",
    "    x = BatchNormalization()(x)\n",
    "prev_label = x\n",
    "\n",
    "# concatenate all features\n",
    "x = Concatenate(axis=-1)([slow_feature, quick_feature, prev_label])\n",
    "if try_normalization:\n",
    "    x = BatchNormalization()(x)\n",
    "concate_feature = x\n",
    "\n",
    "# Further process the output\n",
    "x = LSTM(units=64, dropout=0.2, activation=\"relu\", return_sequences=False)(concate_feature)\n",
    "if try_normalization:\n",
    "    x = BatchNormalization()(x)\n",
    "final_feature = x\n",
    "\n",
    "# Final prediction\n",
    "x = Dense(units=512, activation=\"sigmoid\")(x)\n",
    "if try_normalization:\n",
    "    x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(units=4, activation=\"sigmoid\")(x)\n",
    "\n",
    "# compile model\n",
    "Indexer = Model(inputs=[slow_feature_input, quick_feature_input, prev_label_input], outputs=[output])\n",
    "Indexer.compile(loss=\"mse\", optimizer='adam', metrics=['mse'])\n",
    "\n",
    "# display model\n",
    "plot_model(model=Indexer, to_file=\"temp.png\")\n",
    "Indexer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T21:29:56.312916Z",
     "start_time": "2018-10-11T21:29:03.967421Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2975 samples, validate on 850 samples\n",
      "Epoch 1/300\n",
      "2975/2975 [==============================] - 15s 5ms/step - loss: 0.0583 - mean_squared_error: 0.0583 - val_loss: 5.7940e-04 - val_mean_squared_error: 5.7940e-04\n",
      "Epoch 2/300\n",
      "2975/2975 [==============================] - 13s 4ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 4.6619e-04 - val_mean_squared_error: 4.6619e-04\n",
      "Epoch 3/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d9db82435a27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m              \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_df1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_df2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_df3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m              callbacks=[TensorBoard()])\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m scores = desc_conv_model.evaluate(x=[train_data['test_df1'], train_data['test_df2'], train_data['test_df3']], \n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Indexer.fit([train_data['train_df1'], train_data['train_df2'], train_data['train_df3']], \n",
    "             train_data['train_label'], epochs=300, batch_size=128, shuffle=True,\n",
    "             verbose=1, \n",
    "             validation_data=[[train_data['test_df1'], train_data['test_df2'], train_data['test_df3']], train_data['test_label']],\n",
    "             callbacks=[TensorBoard()])\n",
    "\n",
    "scores = desc_conv_model.evaluate(x=[train_data['test_df1'], train_data['test_df2'], train_data['test_df3']], \n",
    "                                  y=[train_data['test_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1319px",
    "right": "20px",
    "top": "-4px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
